{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "https://www.tensorflow.org/get_started/mnist/pros\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing data to /Users/ariellev/myWS/sandbox/kaggle/notebooks/tensor-flow/data/mnist\n",
      "Extracting /Users/ariellev/myWS/sandbox/kaggle/notebooks/tensor-flow/data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting /Users/ariellev/myWS/sandbox/kaggle/notebooks/tensor-flow/data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting /Users/ariellev/myWS/sandbox/kaggle/notebooks/tensor-flow/data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting /Users/ariellev/myWS/sandbox/kaggle/notebooks/tensor-flow/data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "path = os.getcwd() + \"/data/mnist\"\n",
    "print (\"importing data to %s\" % path)\n",
    "mnist = input_data.read_data_sets(path, one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape [28,28]=784\n"
     ]
    }
   ],
   "source": [
    "image_width = 28\n",
    "image_height = 28\n",
    "image_size = np.multiply(image_width, image_height)\n",
    "print(\"image shape [%s,%s]=%s\" % (image_width, image_height, image_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAA/CAYAAADwizNIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1245d7e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# display image\n",
    "def display(imgs):\n",
    "    n=imgs.size / image_size\n",
    "    i=1\n",
    "    for img in imgs:\n",
    "        # (784) => (28,28)\n",
    "        one_image = img.reshape(image_width,image_height)\n",
    "        plt.subplot(1,n,i)\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.imshow(one_image, cmap=cm.binary)\n",
    "        i=i+1\n",
    "\n",
    "# output images\n",
    "imgs=mnist.train.images[0:10]\n",
    "display(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "epochs = 1\n",
    "batch_size = 100\n",
    "display_epoch = 1\n",
    "notebook = \"mnist-cnn\"\n",
    "logs_path = \"%s/%s/%s\" % (os.getcwd(), notebook, \"logs\")\n",
    "model_path = \"%s/%s/%s\" % (os.getcwd(), notebook, \"model\")\n",
    "checkpoint_path = \"%s/%s/%s\" % (os.getcwd(), notebook, \"checkpoints\")\n",
    "\n",
    "os.makedirs(logs_path,exist_ok=True)\n",
    "os.makedirs(model_path,exist_ok=True)\n",
    "os.makedirs(checkpoint_path,exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# weight initialization\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# convolution\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first layer is a convolution, followed by max pooling. The convolution computes 32 features for each 5x5 patch. Its weight tensor has a shape of [5, 5, 1, 32]. The first two dimensions are the patch size, the next is the number of input channels (1 means that images are grayscale), and the last is the number of output channels. There is also a bias vector with a component for each output channel.\n",
    "\n",
    "\n",
    "To apply the layer, we reshape the input data to a 4d tensor, with the first dimension corresponding to the number of images, second and third - to image width and height, and the final dimension - to the number of colour channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf Graph Input\n",
    "# mnist data image of shape 28*28=784\n",
    "x = tf.placeholder(tf.float32, [None, 784], name='InputData')\n",
    "# 0-9 digits recognition => 10 classes\n",
    "y_ = tf.placeholder(tf.float32, [None, 10], name='LabelData')\n",
    "\n",
    "# first convolutional layer, with max pool\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "#print (h_conv1.get_shape()) # => (40000, 28, 28, 32)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "#print (h_pool1.get_shape()) # => (40000, 14, 14, 32)\n",
    "\n",
    "# second convolutional layer, with max pool\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# densely connected layer\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# drop out\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# readout layer\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "y_softmax = tf.nn.softmax(y_conv, name='y_softmax')\n",
    "values, indices = tf.nn.top_k(y_softmax, 10)\n",
    "\n",
    "table = tf.contrib.lookup.index_to_string_table_from_tensor(tf.constant([str(i) for i in range(10)]))\n",
    "prediction_classes = table.lookup(tf.to_int64(indices))\n",
    "\n",
    "\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"loss\", cross_entropy)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "#tf.summary.histogram(\"prediction\", pred)\n",
    "#tf.summary.histogram(\"weights\", W)\n",
    "\n",
    "# Merge all summaries into a single op\n",
    "summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total batches = 550\n",
      "step 0, training accuracy 0.08\n",
      "step 10, training accuracy 0.22\n",
      "step 20, training accuracy 0.48\n",
      "step 30, training accuracy 0.69\n",
      "step 40, training accuracy 0.64\n",
      "step 50, training accuracy 0.74\n",
      "step 60, training accuracy 0.75\n",
      "step 70, training accuracy 0.83\n",
      "step 80, training accuracy 0.79\n",
      "step 90, training accuracy 0.83\n",
      "step 100, training accuracy 0.84\n",
      "step 110, training accuracy 0.91\n",
      "step 120, training accuracy 0.88\n",
      "step 130, training accuracy 0.91\n",
      "step 140, training accuracy 0.92\n",
      "step 150, training accuracy 0.87\n",
      "step 160, training accuracy 0.91\n",
      "step 170, training accuracy 0.91\n",
      "step 180, training accuracy 0.91\n",
      "step 190, training accuracy 0.87\n",
      "step 200, training accuracy 0.91\n",
      "step 210, training accuracy 0.89\n",
      "step 220, training accuracy 0.91\n",
      "step 230, training accuracy 0.93\n",
      "step 240, training accuracy 0.93\n",
      "step 250, training accuracy 0.93\n",
      "step 260, training accuracy 0.92\n",
      "step 270, training accuracy 0.95\n",
      "step 280, training accuracy 0.95\n",
      "step 290, training accuracy 0.88\n",
      "step 300, training accuracy 0.94\n",
      "step 310, training accuracy 0.95\n",
      "step 320, training accuracy 0.92\n",
      "step 330, training accuracy 0.95\n",
      "step 340, training accuracy 0.96\n",
      "step 350, training accuracy 0.95\n",
      "step 360, training accuracy 0.91\n",
      "step 370, training accuracy 0.95\n",
      "step 380, training accuracy 0.93\n",
      "step 390, training accuracy 0.93\n",
      "step 400, training accuracy 0.91\n",
      "step 410, training accuracy 0.97\n",
      "step 420, training accuracy 0.92\n",
      "step 430, training accuracy 0.97\n",
      "step 440, training accuracy 0.96\n",
      "step 450, training accuracy 0.94\n",
      "step 460, training accuracy 0.95\n",
      "step 470, training accuracy 0.93\n",
      "step 480, training accuracy 0.98\n",
      "step 490, training accuracy 0.93\n",
      "step 500, training accuracy 0.93\n",
      "step 510, training accuracy 0.98\n",
      "step 520, training accuracy 0.93\n",
      "step 530, training accuracy 0.93\n",
      "step 540, training accuracy 0.96\n",
      "step 549, training accuracy 0.96\n",
      "Checkpoint saved to path: /Users/ariellev/myWS/sandbox/kaggle/notebooks/tensor-flow/mnist-cnn/checkpoints/mnist-cnn.ckpt\n",
      "done training mnist-cnn\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: b'/Users/ariellev/myWS/sandbox/kaggle/notebooks/tensor-flow/mnist-cnn/model/saved_model.pb'\n",
      "Model exported to path: /Users/ariellev/myWS/sandbox/kaggle/notebooks/tensor-flow/mnist-cnn/model\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    \n",
    "    total_batches = int(mnist.train.num_examples/batch_size)\n",
    "    print(\"total batches = %s\" % (total_batches))\n",
    "    for e in range(epochs):\n",
    "        for i in range(total_batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size) \n",
    "            if i % 10 == 0:\n",
    "                # record summary data\n",
    "                train_accuracy, summary = sess.run([accuracy, summary_op], feed_dict={x: batch_xs, y_: batch_ys, keep_prob: 1.0})\n",
    "                summary_writer.add_summary(summary, e * total_batch + i)\n",
    "                print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "            else:\n",
    "                _ = sess.run([train_step], feed_dict={x: batch_xs, y_: batch_ys, keep_prob: 0.5})\n",
    "\n",
    "        print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "        p = saver.save(sess, \"%s/%s.ckpt\" % (checkpoint_path, notebook))\n",
    "        print(\"Checkpoint saved to path: %s\" % p)\n",
    "\n",
    "    print('done training mnist-cnn')\n",
    "    #print('test accuracy %g' % accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\n",
    "    \n",
    "    # Build the signature_def_map.\n",
    "    classification_inputs = tf.saved_model.utils.build_tensor_info(x)\n",
    "    classification_outputs_classes = tf.saved_model.utils.build_tensor_info(prediction_classes)\n",
    "    classification_outputs_scores = tf.saved_model.utils.build_tensor_info(values)\n",
    "\n",
    "    classification_signature = (\n",
    "      tf.saved_model.signature_def_utils.build_signature_def(\n",
    "          inputs={\n",
    "              tf.saved_model.signature_constants.CLASSIFY_INPUTS:\n",
    "                  classification_inputs\n",
    "          },\n",
    "          outputs={\n",
    "              tf.saved_model.signature_constants.CLASSIFY_OUTPUT_CLASSES:\n",
    "                  classification_outputs_classes,\n",
    "              tf.saved_model.signature_constants.CLASSIFY_OUTPUT_SCORES:\n",
    "                  classification_outputs_scores\n",
    "          },\n",
    "          method_name=tf.saved_model.signature_constants.CLASSIFY_METHOD_NAME))\n",
    "    \n",
    "    tensor_info_x = tf.saved_model.utils.build_tensor_info(x)\n",
    "    tensor_info_y = tf.saved_model.utils.build_tensor_info(y_softmax)\n",
    "\n",
    "    # build prediction signature\n",
    "    prediction_signature = (\n",
    "          tf.saved_model.signature_def_utils.build_signature_def(\n",
    "              inputs={'images': tensor_info_x},\n",
    "              outputs={'scores': tensor_info_y},\n",
    "              method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME))\n",
    "\n",
    "    # saving model\n",
    "    legacy_init_op = tf.group(tf.tables_initializer(), name='legacy_init_op')\n",
    "\n",
    "    if os.path.isdir(model_path):\n",
    "        shutil.rmtree(model_path)\n",
    "        \n",
    "    builder = tf.saved_model.builder.SavedModelBuilder(model_path)\n",
    "    builder.add_meta_graph_and_variables(\n",
    "        sess,[tf.saved_model.tag_constants.SERVING],\n",
    "        signature_def_map={\n",
    "          'predict_images':\n",
    "              prediction_signature,\n",
    "          tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:\n",
    "              classification_signature,\n",
    "        },\n",
    "        legacy_init_op=legacy_init_op)\n",
    "    \n",
    "    \n",
    "    builder.save()\n",
    "    print(\"Model exported to path: %s\" % model_path)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
